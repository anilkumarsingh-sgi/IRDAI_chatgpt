# ğŸ›ï¸ IRDAI Compliance GPT â€” Deployment Guide

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Streamlit Cloud                          â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Crawler  â”‚  â”‚Ingestion â”‚  â”‚   Streamlit UI (app.py)   â”‚ â”‚
â”‚  â”‚(crawler) â”‚â†’ â”‚(ingest)  â”‚â†’ â”‚   + RAG Pipeline          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚              â”‚                   â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SQLite  â”‚   â”‚  ChromaDB â”‚  â”‚  HuggingFace API        â”‚ â”‚
â”‚  â”‚(tracker) â”‚   â”‚  (vectors)â”‚  â”‚  Mistral-7B-Instruct    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â–²                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  scheduler.py â€“ Background thread (every 12 hours)   â”‚   â”‚
â”‚  â”‚  Auto-crawls IRDAI â†’ downloads new docs â†’ ingests    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Folder Structure

```
irdai_compliance_gpt/
â”œâ”€â”€ app.py                  â† Streamlit UI + RAG pipeline
â”œâ”€â”€ crawler.py              â† IRDAI website crawler
â”œâ”€â”€ ingestion.py            â† PDF/Excel/Word â†’ embed â†’ ChromaDB
â”œâ”€â”€ scheduler.py            â† Background auto-update scheduler
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ packages.txt            â† System packages for Streamlit Cloud
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .streamlit/
â”‚   â”œâ”€â”€ config.toml         â† Streamlit theme/server config
â”‚   â””â”€â”€ secrets.toml        â† HF_TOKEN (NEVER commit!)
â””â”€â”€ data/                   â† Auto-created at runtime
    â”œâ”€â”€ pdfs/               â† Downloaded PDFs by category
    â”œâ”€â”€ excel/              â† Downloaded Excel files
    â”œâ”€â”€ word/               â† Downloaded Word docs
    â”œâ”€â”€ chroma_db/          â† Vector store
    â””â”€â”€ scheduler_state.jsonâ† Auto-update state tracker
```

---

## ğŸš€ A) Step-by-Step Deployment Guide

### Step 1: Get HuggingFace Token
1. Go to https://huggingface.co/settings/tokens
2. Create a new token with **Read** permission
3. Copy the token (starts with `hf_`)

### Step 2: Clone & Set Up Locally (optional test)
```bash
git clone https://github.com/YOUR_USERNAME/irdai-compliance-gpt.git
cd irdai-compliance-gpt
pip install -r requirements.txt
cp .env.example .env          # fill in HF_TOKEN
streamlit run app.py
```

### Step 3: Push to GitHub
```bash
git init
git add app.py crawler.py ingestion.py scheduler.py requirements.txt packages.txt .streamlit/config.toml .gitignore README.md
# DO NOT add data/ or secrets.toml
git commit -m "IRDAI Compliance GPT with auto-update"
git remote add origin https://github.com/YOUR_USERNAME/irdai-compliance-gpt.git
git push -u origin main
```

---

## ğŸŒ B) Streamlit Cloud Deployment

1. Go to https://share.streamlit.io
2. Click **"New app"**
3. Select your GitHub repo and branch
4. Set **Main file path** to `app.py`
5. Click **"Advanced settings"**
6. Add secrets:
   ```toml
   HF_TOKEN = "hf_your_actual_token_here"
   ```
7. (Optional) Set environment variable for update interval:
   ```toml
   # In secrets or as env var â€” interval in seconds (default: 43200 = 12 hours)
   IRDAI_UPDATE_INTERVAL = "43200"
   ```
8. Click **Deploy**

> â³ First deployment takes ~5 minutes (installs packages)

---

## ğŸ”„ C) Automatic Document Updates

The app includes a **background scheduler** that automatically:

1. **Crawls IRDAI website** every 12 hours (configurable via `IRDAI_UPDATE_INTERVAL`)
2. **Downloads new PDFs, Excel & Word** documents with deduplication
3. **Ingests new documents** into ChromaDB vector store
4. **Tracks update state** â€” shows last update time in the sidebar

### How it works on Streamlit Cloud:
- A daemon thread starts when the app boots
- It checks every 5 minutes if an update is due
- When due, it runs the full crawl â†’ ingest pipeline in the background
- The UI shows real-time status: Running / Last updated X hours ago / Pending
- **Manual override**: Click "ğŸ”„ Force Update Now" in the sidebar

### Important: Ephemeral Storage
- On Streamlit Cloud, `/tmp/irdai_data/` is used (ephemeral â€” resets on reboot)
- **On first start**, the scheduler will automatically crawl and build the vector database
- Subsequent restarts will re-crawl (data is fresh but takes a few minutes to rebuild)
- For persistent storage, consider upgrading to a cloud database (see Enterprise section)

---

## âš¡ D) Running the Pipeline Manually

The sidebar includes **Admin Actions**:

1. **ğŸ”„ Force Update Now** â€” Triggers immediate crawl + ingestion in background
2. **ğŸ•·ï¸ Run Crawler** â€” Crawls IRDAI website only (downloads new documents)
3. **ğŸ“¥ Run Ingestion** â€” Processes downloaded docs into ChromaDB only

---

## ğŸ“‰ E) Handling HuggingFace API Limits

| Plan | Rate Limit | Notes |
|------|-----------|-------|
| Free | ~30 req/min | Good for demos |
| PRO ($9/mo) | ~300 req/min | 10-20 users |
| Enterprise | Custom | Production |

The app automatically:
- Detects 429 rate limit errors
- Displays a user-friendly retry message
- Caches the LLM client with `@st.cache_resource`

---

## ğŸ’° F) Cost Optimization Tips

1. **Use smaller models first**: `google/flan-t5-large` is free and fast for simple queries
2. **Cache aggressively**: Use `@st.cache_resource` for models, `@st.cache_data` for static data
3. **Limit n_results**: Keep retrieval to 3-5 chunks (reduces context size â†’ cheaper API calls)
4. **Auto-scraping eliminates manual work**: The scheduler handles updates automatically
5. **Use quantized models**: `TheBloke/Mistral-7B-Instruct-v0.2-GPTQ` is faster on shared infra
6. **Tune update interval**: Set `IRDAI_UPDATE_INTERVAL` to `86400` (24h) if 12h is too frequent

---

## ğŸ¢ G) Enterprise Upgrade Plan

| Feature | Free/Cloud | Enterprise |
|---------|-----------|------------|
| Hosting | Streamlit Cloud | AWS/GCP/Azure |
| LLM | HF Inference API | Dedicated HF Endpoint / Azure OpenAI |
| Vector DB | Local ChromaDB | Pinecone / Weaviate / pgvector |
| Storage | Ephemeral | S3 + persistent EFS |
| Auth | None | SSO / LDAP |
| Audit Logs | None | Full audit trail |
| Multi-tenant | No | Yes |
| SLA | None | 99.9% |

---

## ğŸ” Security Hardening Checklist

- [x] HF_TOKEN stored in Streamlit secrets (never in code)
- [x] No policyholder data in the system
- [x] Only public IRDAI documents processed
- [x] Disclaimer displayed prominently in UI
- [ ] Add IP allowlisting (enterprise)
- [ ] Enable Streamlit authentication (enterprise)
- [ ] Run ChromaDB on separate persistence layer
- [ ] Add query logging + audit trail for compliance
- [ ] Rotate HF tokens quarterly
- [ ] Network-level isolation (VPC) for enterprise deploy

---

## ğŸ“ Support

For issues, open a GitHub issue or contact your AI platform team.
