# üèõÔ∏è IRDAI Compliance GPT ‚Äî Deployment Guide

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Streamlit Cloud                     ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Crawler  ‚îÇ  ‚îÇIngestion ‚îÇ  ‚îÇ   Streamlit UI (app) ‚îÇ ‚îÇ
‚îÇ  ‚îÇ(crawler) ‚îÇ  ‚îÇ(ingest)  ‚îÇ  ‚îÇ   + RAG Pipeline     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ              ‚îÇ                   ‚îÇ              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  SQLite  ‚îÇ   ‚îÇ  ChromaDB ‚îÇ  ‚îÇ  HuggingFace API   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ(tracker) ‚îÇ   ‚îÇ  (vectors)‚îÇ  ‚îÇ  Mistral-7B-Instruct‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Folder Structure

```
irdai_compliance_gpt/
‚îú‚îÄ‚îÄ app.py                  ‚Üê Streamlit UI + RAG pipeline
‚îú‚îÄ‚îÄ crawler.py              ‚Üê IRDAI website crawler
‚îú‚îÄ‚îÄ ingestion.py            ‚Üê PDF ‚Üí embed ‚Üí ChromaDB
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml        ‚Üê HF_TOKEN (don't commit!)
‚îî‚îÄ‚îÄ data/                   ‚Üê Auto-created at runtime
    ‚îú‚îÄ‚îÄ pdfs/               ‚Üê Downloaded PDFs
    ‚îú‚îÄ‚îÄ chroma_db/          ‚Üê Vector store
    ‚îî‚îÄ‚îÄ irdai_tracker.db    ‚Üê SQLite dedup tracker
```

---

## üöÄ A) Step-by-Step Deployment Guide

### Step 1: Get HuggingFace Token
1. Go to https://huggingface.co/settings/tokens
2. Create a new token with **Read** permission
3. Copy the token (starts with `hf_`)

### Step 2: Clone & Set Up Locally (optional test)
```bash
git clone https://github.com/YOUR_USERNAME/irdai-compliance-gpt.git
cd irdai-compliance-gpt
pip install -r requirements.txt
cp .env.example .env          # fill in HF_TOKEN
streamlit run app.py
```

### Step 3: Push to GitHub
```bash
git init
git add app.py crawler.py ingestion.py requirements.txt .streamlit/ .env.example
# DO NOT add data/ or .env (add them to .gitignore)
echo "data/" >> .gitignore
echo ".env" >> .gitignore
git commit -m "Initial IRDAI Compliance GPT"
git remote add origin https://github.com/YOUR_USERNAME/irdai-compliance-gpt.git
git push -u origin main
```

---

## üåê B) Streamlit Cloud Deployment

1. Go to https://share.streamlit.io
2. Click **"New app"**
3. Select your GitHub repo and branch
4. Set **Main file path** to `app.py`
5. Click **"Advanced settings"**
6. Add secrets:
   ```toml
   HF_TOKEN = "hf_your_actual_token_here"
   ```
7. Click **Deploy**

> ‚è≥ First deployment takes ~5 minutes (installs packages)

---

## ‚ö° C) Running the Pipeline on Streamlit Cloud

The app includes **Admin Actions** in the sidebar:

1. **üï∑Ô∏è Run Crawler** ‚Äî Crawls IRDAI website, downloads PDFs into `data/pdfs/`
2. **üì• Run Ingestion** ‚Äî Processes PDFs, generates embeddings, stores in ChromaDB

> **Note:** On Streamlit Cloud, `data/` is ephemeral (resets on redeploy). For persistence, use a mounted volume or S3 + pre-built ChromaDB.

---

## üìâ D) Handling HuggingFace API Limits

| Plan | Rate Limit | Notes |
|------|-----------|-------|
| Free | ~30 req/min | Good for demos |
| PRO ($9/mo) | ~300 req/min | 10-20 users |
| Enterprise | Custom | Production |

The app automatically:
- Detects 429 rate limit errors
- Displays a user-friendly retry message
- Caches the LLM client with `@st.cache_resource`

---

## üí∞ E) Cost Optimization Tips

1. **Use smaller models first**: `google/flan-t5-large` is free and fast for simple queries
2. **Cache aggressively**: Use `@st.cache_resource` for models, `@st.cache_data` for static data
3. **Limit n_results**: Keep retrieval to 3-5 chunks (reduces context size ‚Üí cheaper API calls)
4. **Batch ingestion**: Run crawler + ingestion once a week, not on every visit
5. **Use quantized models**: `TheBloke/Mistral-7B-Instruct-v0.2-GPTQ` is faster on shared infra

---

## üè¢ F) Enterprise Upgrade Plan

| Feature | Free/Cloud | Enterprise |
|---------|-----------|------------|
| Hosting | Streamlit Cloud | AWS/GCP/Azure |
| LLM | HF Inference API | Dedicated HF Endpoint / Azure OpenAI |
| Vector DB | Local ChromaDB | Pinecone / Weaviate / pgvector |
| Storage | Ephemeral | S3 + persistent EFS |
| Auth | None | SSO / LDAP |
| Audit Logs | None | Full audit trail |
| Multi-tenant | No | Yes |
| SLA | None | 99.9% |

---

## üîê Security Hardening Checklist

- [x] HF_TOKEN stored in Streamlit secrets (never in code)
- [x] No policyholder data in the system
- [x] Only public IRDAI documents processed
- [x] Disclaimer displayed prominently in UI
- [ ] Add IP allowlisting (enterprise)
- [ ] Enable Streamlit authentication (enterprise)
- [ ] Run ChromaDB on separate persistence layer
- [ ] Add query logging + audit trail for compliance
- [ ] Rotate HF tokens quarterly
- [ ] Network-level isolation (VPC) for enterprise deploy

---

## üìû Support

For issues, open a GitHub issue or contact your AI platform team.
